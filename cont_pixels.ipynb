{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.plot import show, show_hist\n",
    "import warnings\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import io\n",
    "import rasterio\n",
    "from rasterio.io import MemoryFile\n",
    "from itertools import islice\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrélation de Moran sur l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def compute_moran_corr(plume_dir, no_plume_dir, save_plume_dir, save_no_plume_dir, window_size):\n",
    "\n",
    "    all_files_no_plume = os.listdir(no_plume_dir)\n",
    "    tif_files = [filename for filename in all_files_no_plume if filename.endswith(\".tif\")]\n",
    "\n",
    "    for filename in tif_files:\n",
    "        img_path = os.path.join(no_plume_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        gray_image = np.array(img)\n",
    "\n",
    "        weights = np.ones((window_size, window_size))  # Poids uniformes pour une fenêtre carrée\n",
    "\n",
    "        # Normaliser les poids pour que leur somme soit égale à 1\n",
    "        weights /= np.sum(weights)\n",
    "\n",
    "        # Appliquer la corrélation de Moran\n",
    "        moran_auto_corr = signal.convolve2d(gray_image, weights, mode='same')\n",
    "        \n",
    "        \n",
    "        save_path = save_no_plume_dir+filename\n",
    "        image_to_save = Image.fromarray(moran_auto_corr)   \n",
    "        image_to_save.save(save_path)\n",
    "\n",
    "    all_files_plume = os.listdir(plume_dir)\n",
    "    tif_files = [filename for filename in all_files_plume if filename.endswith(\".tif\")]\n",
    "\n",
    "    for filename in tif_files:\n",
    "        img_path = os.path.join(plume_dir, filename)\n",
    "        img = Image.open(img_path)\n",
    "        gray_image = np.array(img)\n",
    "\n",
    "        weights = np.ones((window_size, window_size))  # Poids uniformes pour une fenêtre carrée\n",
    "\n",
    "        # Normaliser les poids pour que leur somme soit égale à 1\n",
    "        weights /= np.sum(weights)\n",
    "\n",
    "        # Appliquer la corrélation de Moran\n",
    "        moran_auto_corr = signal.convolve2d(gray_image, weights, mode='same')\n",
    "        \n",
    "        save_path = save_plume_dir+filename\n",
    "        image_to_save = Image.fromarray(moran_auto_corr)   \n",
    "        image_to_save.save(save_path)\n",
    "    \n",
    "    return('done')\n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_entropy(image):\n",
    "    # Convertir l'image en niveaux de gris si ce n'est pas déjà le cas\n",
    "    if image.ndim == 3:\n",
    "        image = np.mean(image, axis=2)\n",
    "\n",
    "    # Calculer l'histogramme des niveaux de gris\n",
    "    histogram, _ = np.histogram(image, bins=256, range=(0, 256))\n",
    "\n",
    "    # Normaliser l'histogramme pour obtenir une distribution de probabilité\n",
    "    histogram = histogram / float(np.sum(histogram))\n",
    "\n",
    "    # Calculer l'entropie en utilisant la formule de Shannon\n",
    "    entropy = -np.sum(histogram * np.log2(histogram + np.finfo(float).eps))\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "# returns a list of entropy calculated for each image of images_dir, which file is in metadata.path column\n",
    "def compute_entropy_list(train_data_dir, metadata_dir):\n",
    "    train_data_dir = train_data_dir\n",
    "    metadata = pd.read_csv(metadata_dir)\n",
    "\n",
    "    entropy_list = []\n",
    "\n",
    "    for filename in metadata.path:\n",
    "        img_path = images_dir+filename+'.tif'\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "\n",
    "    \n",
    "        entropy_value = image_entropy(img_array)\n",
    "        entropy_list.append(entropy_value)\n",
    "\n",
    "    return(entropy_list)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_skewness(image):\n",
    "    # Convertir l'image en niveaux de gris si ce n'est pas déjà le cas\n",
    "\n",
    "    # Calculer la skewness des niveaux de gris\n",
    "    skewness = skew(image, axis=None)\n",
    "\n",
    "    return skewness\n",
    "\n",
    "def compute_skewness_list(train_data_dir, metadata_dir):\n",
    "    train_data_dir = train_data_dir\n",
    "    metadata = pd.read_csv(metadata_dir)\n",
    "\n",
    "    skewness_list = []\n",
    "\n",
    "    for filename in metadata.path:\n",
    "        img_path = images_dir+filename+'.tif'\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "\n",
    "    \n",
    "        skewness_value = image_skewness(img_array)\n",
    "        skewness_list.append(skewness_value)\n",
    "\n",
    "    return(skewness_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_kurtosis(image):\n",
    "    # Convertir l'image en niveaux de gris si ce n'est pas déjà le cas\n",
    "    if image.ndim == 3:\n",
    "        image = np.mean(image, axis=2)\n",
    "\n",
    "    # Calculer le kurtosis des niveaux de gris\n",
    "    kurtosis_value = kurtosis(image, axis=None)\n",
    "\n",
    "    return kurtosis_value\n",
    "\n",
    "def compute_kurtosis_list(train_data_dir, metadata_dir):\n",
    "    train_data_dir = train_data_dir\n",
    "    metadata = pd.read_csv(metadata_dir)\n",
    "\n",
    "    kurtosis_list = []\n",
    "\n",
    "    for filename in metadata.path:\n",
    "        img_path = images_dir+filename+'.tif'\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "\n",
    "    \n",
    "        kurtosis_value = image_kurtosis(img_array)\n",
    "        kurtosis_list.append(kurtosis_value)\n",
    "\n",
    "    return(kurtosis_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask & plume parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_parameters(train_data_dir, metadata_dir):\n",
    "    train_data_dir = train_data_dir\n",
    "    metadata = pd.read_csv(metadata_dir)\n",
    "\n",
    "    width_list = []\n",
    "    height_list = []\n",
    "    area_list = []\n",
    "    centroid_x_list = []\n",
    "    centroid_y_list = []\n",
    "\n",
    "    for filename in metadata.path:\n",
    "        img_path = images_dir+filename+'.tif'\n",
    "        img = Image.open(img_path)\n",
    "        image = np.array(img)\n",
    "\n",
    "        max_value = np.max(image)\n",
    "\n",
    "        ## create the mask regarding the max intensity of the image, the threshold\n",
    "        seuil = 0.9*max_value\n",
    "        resultat = np.where(image < seuil, 0, 1)\n",
    "        resultat = resultat.astype(np.uint8)\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(resultat, connectivity=4)\n",
    "\n",
    "        plus_grande_aire = 0\n",
    "        label_plus_grande_aire = None\n",
    "\n",
    "        for label in range(1, num_labels):  \n",
    "            left, top, width, height, area = stats[label]\n",
    "            \n",
    "            if area > plus_grande_aire:\n",
    "                plus_grande_aire = area\n",
    "                label_plus_grande_aire = label\n",
    "\n",
    "        # Vérifie si au moins une composante connexe a été trouvée\n",
    "        if label_plus_grande_aire is not None:\n",
    "            left, top, width, height, area = stats[label_plus_grande_aire]\n",
    "            centroid_x, centroid_y = centroids[label_plus_grande_aire]\n",
    "            width_list.append(width)\n",
    "            height_list.append(height)\n",
    "            area_list.append(area)\n",
    "            centroid_x_list.append(centroid_x)\n",
    "            centroid_y_list.append(centroid_y)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"Aucune composante connexe n'a été trouvée.\")\n",
    "\n",
    "    return(width_list, height_list, area_list, centroid_x_list, centroid_y_list)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
